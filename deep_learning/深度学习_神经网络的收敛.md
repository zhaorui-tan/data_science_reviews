# 神经网络最终收敛何处？

## 局部最小值
很容易想到，收敛过程中遇到的第一个阻碍就是“局部最小值点”。

从数学的角度来说，局部最小值点首先满足一阶导数均为0， 我们还要保证曲线是凹的，也就是说，局部最小值点二阶导数均大于0。 总结来说，局部最小值满足两个条件：

- 对于所有参数的一阶导数均为0.
- 对于所有参数的二阶参数均大于0.

我们从直观的感觉来看，我们陷入局部最小值点的可能性还是很小的（根据损失函数不同而有所不同），且对于陷入局部最小值点，我们有很多方案来“跳出”该点

## 鞍点
损失函数中遇到的第二个阻碍便是“鞍点”。

从数学角度来说，鞍点同样满足两个条件：

- 对于所有参数的一阶导数均为0.
- 存在对于某一个参数，其二阶导数小于0.


实际上，在损失函数收敛过程中，遇到鞍点的概率要比“局部最小值点”要大的多。怎么说呢，我们比较一下上面对于“局部最小值”以及“鞍点”的定义，假设在一阶导数全为0的情况下，二阶导数全大于0与小于0的概率均为0.5，假设此时有1000个参数，那么该点是“局部最小值”点的概率为 0.5^1000 ， 该点为“局部最大值点”的概率为 0.5^1000 ，而该点为鞍点的概率为： 1-2*0.5^1000 。

因此，我们可以得出结论：如果我们的模型最终收敛到一个一阶导数全为0的点，那么这个点大概率是鞍点。而实际上我们也几乎不会收敛到“鞍点”，且“鞍点”也是极容易逃出的。

我们假设，对于任意参数，我们能得到一阶导数为0的概率为0.1，那么假设我们现在有50个参数，我们踩到鞍点的概率为 [0.1^50 ，这个概率实在是太低了，我们几乎不可能碰到。

## 大片平滑区
在实际的高山中，我们能看到高耸入云，陡峭如刀的山峰，也能看到高且平缓，绵延不绝的山路，如果你从山顶扔出一个球，这个球往往在陡峭时下降快速，在平缓时下降缓慢。

损失函数往往就像山峰那样，有的高耸，有的平缓，而我们通常希望能够定义一个陡峭的损失函数来加速收敛过程，但事实总非如我们所愿。

如果在一个损失函数中，有着许多大片的平坦区域，这对于收敛简直是一个噩梦，实际优化过程中，我们通常在梯度变化很小的一定batch次数内终止收敛过程，而如果我们陷入在这样的平坦区域，但是没有设置足够的时间来逃出这块平坦区域，那么，我们最终收敛在这块区域的可能性就变得很大，而如果这块区域刚好在loss很高的区域，那么我们也就很难得到很好的结果了。

## 如何跳出局部最小值？


- 以多组不同的初始化方式来舒适化神经网络，然后选择结果最好的。 这相当于你在下山时换了一个初始点，这样就有可能会获得更加接近全局最小值的结果。
- 采用“随机梯度下降算法”（mini-batch也可以）来增加随机性。这样会带来一个效果，即使我们陷入了局部最小点，其计算的梯度仍可能部位0，且即使此时为0，我们重复多个epoch，就很容易跳出局部最小值了。
- 模拟退火算法。该算法实际中应用并不常见。

## 如何跳出鞍点 ？
答案是选择合适的优化算法。优化算法比你想象的要多，也更重要一些，这属于经验科学，使用的多了，见到的情景多了，自然而言就能够理清各种优化算法之间的优劣。

说到底，深度学习依旧是一个实验学科，实验才是王道，做的实验多了，自然明了。

## 如何避免陷入局部最小值与鞍点？
- **SGD 或 Mini-batch**：SGD 与 Mini-batch 引入了随机性，每次以部分样本来计算梯度，能够相当程度上避免陷入局部最小值。
- **动量**： 引入动量，相当于引入惯性。一些常见情况时，如上次梯度过大，导致进入局部最小点时，下一次更新能很容易借助上次的大梯度跳出局部最小点。
- **自适应学习率**：通过学习率来控制梯度是一个很棒的思想， 自适应学习率算法能够基于历史的累计梯度去计算一个当前较优的学习率。

## 最后
全局最小值点问题是探究神经网络黑匣子的一个重要方面，目前神经网络理论层面太过浅薄使得这些深层次问题依旧没有很好的解答。

总的来说，我认为思考全局最小值问题不如更多的去关注以下几个方面：

- 损失函数的选择 -- 我认为最重要
- 优化算法
- 初始化方案
- 正则化方案
- 神经网络结构以及激活函数
- 各种Trick
