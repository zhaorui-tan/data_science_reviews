[TOC]

# 卷积层

卷积操作原理上其实是对两个矩阵进行点乘求和的数学操作，其中一个矩阵为输入的数据矩阵，另一个矩阵则为卷积核（滤波器或特征矩阵），求得的结果表示为原始图像中提取的特定局部特征。

## 卷积的作用
在图像领域中，深层卷积已经证明比浅层卷积更具表征，从图像的特征提取来看， 不同卷积操作提取到的特征类型是不相同的：

卷积层次  |	特征类型
---|---
浅层卷积|	边缘特征
中层卷积|	局部特征
深层卷积|	全局特征

## 一些常见的卷积核
在传统图像领域，已经证明一些特殊的卷积核能够执行边缘检测，锐化，模糊等操作， 如下表所示：

卷积作用 |	卷积核
---|---
输出原图 |	`$\begin{bmatrix} 0 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 0 \end{bmatrix}$`	
边缘检测（突出边缘差异）|`$\begin{bmatrix} 1 & 0 & -1 \\ 0 & 0 & 0 \\ -1 & 0 & 1 \end{bmatrix}$`
边缘检测（突出中间值）|	`$\begin{bmatrix} -1 & -1 & -1 \\-1 & 8 & -1 \\ -1 & -1 & -1 \end{bmatrix}$`	
图像锐化|	`$\begin{bmatrix} 0 & -1 & 0 \\ -1 & 5 & -1 \\ 0 & -1 & 0 \end{bmatrix}$`	
方块模糊|	`$\begin{bmatrix} 1 & 1 & 1 \\ 1 & 1 & 1 \\ 1 & 1 & 1 \end{bmatrix} \times \frac{1}{9}$`	
高斯模糊|	`$\begin{bmatrix} 1 & 2 & 1 \\ 2 & 4 & 2 \\ 1 & 2 & 1 \end{bmatrix} \times \frac{1}{16}$`	


## 卷积核的参数
参数名|	作用|	常见设置
---|---|---
Kernel Size|卷积核的大小|在过去常设为5，如LeNet-5；现在多设为3，通过堆叠`$3\times3$`的卷积核来达到更大的感受域
Stride|	卷积步长|常见设置为1，表示滑窗距离为1，可以覆盖所有相邻位置特征的组合；当设置为更大值时相当于对特征组合降采样
Padding	|填充策略|SAME：表示对不足卷积核大小的边界位置进行某种填充（通常零填充）以保证卷积输出维度与与输入维度一致；VALID时则对不足卷积尺寸的部分进行舍弃，输出维度就无法保证与输入维度一致
In Channels	|卷积核的深度|默认与输入的特征矩阵通道数（深度）一致；在某些压缩模型中会采用通道分离的卷积方式
Out Channels|	卷积核的个数|若设置为与输入通道数一样的大小，可以保持输入输出维度的一致性；若采用比输入通道数更小的值，则可以减少整体网络的参数量

## 几种常见的卷积方式 

### 空洞卷积
空洞卷积（atrous convolutions）又名扩张卷积（dilated convolutions），向卷积层引入了一个称为 “扩张率(dilation rate)”的新参数，该参数定义了卷积核处理数据时各值的间距。


：
```math
卷积核为3、无扩张

\begin{bmatrix}\#&\#&\#\\\#&\#&\#\\\#&\#&\#\end{bmatrix}

卷积核为3、扩张率为2和无边界扩充的二维空洞卷积

\begin{bmatrix}\#&-&\#&-&\#\\\#&-&\#&-&\#\\\#&-&\#&-&\#\end{bmatrix}
``` 
''''
一个扩张率为2的3×3卷积核，感受野与5×5的卷积核相同，而且仅需要9个参数。你可以把它想象成一个5×5的卷积核，每隔一行或一列删除一行或一列。

#### 适用情况

在相同的计算条件下，空洞卷积提供了更大的感受野。空洞卷积经常用在实时图像分割中。当网络层需要较大的感受野，但计算资源有限而无法提高卷积核数量或大小时，可以考虑空洞卷积。


### 转置卷积（transposed Convolutions）
又名反卷积（deconvolution）或是分数步长卷积（fractially straced convolutions）。

反卷积（deconvolutions）这种叫法是不合适的，因为它不符合反卷积的概念。在深度学习中，反卷积确实存在，但是并不常用。实际上，反卷积是卷积操作的逆过程。你可以这么理解这个过程，将某个图像输入到单个卷积层，取卷积层的输出传递到一个黑盒子中，这个黑盒子输出了原始图像。那么可以说，这个黑盒子完成了一个反卷积操作，也就是卷积操作的数学逆过程。

转置卷积与真正的反卷积有点相似，因为两者产生了相同的空间分辨率。然而，这两种卷积对输入数据执行的实际数学运算是不同的。转置卷积层只执行了常规的卷积操作，但是恢复了其空间分辨率。

#### 例子
举个例子，假如将一张5×5大小的图像输入到卷积层，其中步幅为2，卷积核为3×3，无边界扩充。则卷积层会输出2×2的图像。
若要实现其逆过程，需要相应的数学逆运算，能根据每个输入像素来生成对应的9个值。然后，将步幅设为2，遍历输出图像，这就是反卷积操作。

#### 转置卷积和反卷积
共同点：

转置卷积和反卷积的唯一共同点在于两者输出都为5×5大小的图像，

不同点：

不过转置卷积执行的仍是常规的卷积操作。为了实现扩充目的，需要对输入以某种方式进行填充。你可以理解成，至少在数值方面上，转置卷积不能实现卷积操作的逆过程。

目的：

转置卷积只是为了重建先前的空间分辨率，执行了卷积操作。这不是卷积的数学逆过程，但是用于编码器-解码器结构中，效果仍然很好。这样，转置卷积可以同时实现图像的粗粒化和卷积操作，而不是通过两个单独过程来完成。


### 可分离卷积

#### 可分离卷积
在可分离卷积（separable convolution）中，可将卷积核操作拆分成多个步骤。卷积操作用y=conv(x, k)来表示，其中输出图像为y，输入图像为x，卷积核为k。接着，假设k可以由下式计算得出：k=k1.dot(k2)。这就实现了一个可分离卷积操作，因为不用k执行二维卷积操作，而是通过k1和k2分别实现两次一维卷积来取得相同效果。

Sobel算子通常被用于图像处理中，这里以它为例。你可以分别乘以矢量[1,0,-1]和[1,2,1]的转置矢量后得到相同的滤波器。完成这个操作，只需要6个参数，而不是二维卷积中的9个参数。

这个例子说明了什么叫做空间可分离卷积，这种方法并不应用在深度学习中，只是用来帮你理解这种结构。

#### 可分离卷积结构
在神经网络中，我们通常会使用深度可分离卷积结构（depthwise separable convolution）。

##### 例子
这种方法在保持通道分离的前提下，接上一个深度卷积结构，即可实现空间卷积。接下来通过一个例子让大家更好地理解。

假设有一个3×3大小的卷积层，其输入通道为16、输出通道为32。具体为，32个3×3大小的卷积核会遍历16个通道中的每个数据，从而产生16×32=512个特征图谱。进而通过叠加每个输入通道对应的特征图谱后融合得到1个特征图谱。最后可得到所需的32个输出通道。

针对这个例子应用深度可分离卷积，用1个3×3大小的卷积核遍历16通道的数据，得到了16个特征图谱。在融合操作之前，接着用32个1×1大小的卷积核遍历这16个特征图谱，进行相加融合。这个过程使用了16×3×3+16×32×1×1=656个参数，远少于上面的16×32×3×3=4608个参数。

这个例子就是深度可分离卷积的具体操作，其中上面的深度乘数（depth multiplier）设为1，这也是目前这类网络层的通用参数。

##### 目的

这么做是为了对空间信息和深度信息进行去耦。从Xception模型的效果可以看出，这种方法是比较有效的。由于能够有效利用参数，因此深度可分离卷积也可以用于移动设备中。

### 卷积核的选择
大卷积核虽然可以获取更大的感受域，但是大卷积核反而会导致计算量大幅增加，不利于训练更深层的模型，相应的计算能力也会降低。

堆叠2个 3×3 卷积核（二通道）可以获得与 5×5卷积核 相同的感受视野，同时参数量会更少（3×3×2+1<5×5×1+1）,这也是 3×3卷积 应用更广泛的原因，在大多数情况下通过堆叠较小的卷积核比直接采用单个更大的卷积核会更加有效。


# 激活层
激活层本质就是采用激活函数对卷积出的特征做一个非线性变换。

- 首选 Relu， 然后试试 Relu 变体 Leaky Relu 和 Maxout。
- 某些情况下 tanh 也能获得不错结果。

# 池化层 -- 需要补充
池化层的作用是对感受域内的特征进行筛选，提取区域内最具代表性的特征，能够有效地降低特征尺度，进而减少模型所需要的参数量，此外还可以防止过拟合现象。池化操作的本质是降采样。其除了能显著降低参数数量外，还能保持对平移，伸缩，旋转操作的不变性。

### 常见的池化操作：
- 最大池化： Max Pooling
- 平均值池化： Mean Pooling
无论max pooling还是mean pooling,都没有需要学习的参数。因此，在卷积神经网络的训练中，Pooling层需要做的仅仅是将误差层传递到上一层，而没有梯度的计算。

对于max Pooling，下一层的误差项的值会原封不动的传递到上一层对应区块中的最大值所对应的神经元，而其他神经元的误差项都是0。

对于 mean Pooling， 下一层的误差项会均匀划分到该层的所有的神经元上。

# 卷积层与池化层比较
—|卷积层|	池化层
---|---|---
结构|零填充时输出维度不变，而通道数改变|通常特征维度会降低，通道数不变
稳定性|	输入特征发生细微改变时，输出结果会改变|	感受域内的细微变化不影响输出结果
作用|	感受域内提取局部关联特征|	感受域内提取泛化特征，降低维度
参数量|	与卷积核尺寸、卷积核个数相关|	不引入额外参数

# NLP 与 CV 中使用 CNN 的区别
—|NLP|CV
---|---|---
卷积核|	多为一维卷积， 通常都是由较为浅层的卷积层组成	|对二维信号做卷积，一般设为叠加的3×3卷积核
Pooling	|一般采用 Max-Pooling	|—
全连接层|	一般常采用LN + dropout	|一般常采用BN + dropout

LN：Layer Normalization，LN是“横”着来的，对一个样本，不同的神经元neuron间做归一化。

BN：Batch Normalization，BN是“竖”着来的，各个维度做归一化，所以与batch size有关系。


二者提出的目的都是为了加快模型收敛，减少训练时间。