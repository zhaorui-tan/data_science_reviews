[TOC]

# 线性回归
## 简介
简单来说，线性回归算法就是找到一条直线（一元线性回归）或一个平面（多元线性回归）能够根据输入的特征向量来更好的预测输出y的值。

其本质含义在于 X 与 Y 是线性相关的。 `$ y = \theta_0 + \theta_1x_1 + \cdots + \theta_px_p = \theta^Tx $`


## 线性回归的训练
在线性回归中， 我们可以通过两种方法来求取参数 `$\theta$` ， 一种是采用正规方程(OLS)， 一种是采用梯度下降方法。

其损失函数为
```math
J(\theta) = \frac{1}{2m} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2,  \ 或 \ 矩阵表示: J(\theta) = \frac{1}{2m} (X\theta-y)^T(X\theta - y) 
```

### 正规方程
我们使用 `$J(\theta) $`对 `$\theta$` 求导

得到： `$ \frac{\delta J(\theta)}{\delta \theta} = 2 X^T(X\theta - y) $` 

令上式为0，我们可以得到 `$ \theta$` 的值为： `$ \theta = (X^TX)^{-1}X^Ty $` 

我们可以直接通过矩阵运算来求出参数 `$\theta$` 的解。

而上式我们发现其涉及到了矩阵的可逆问题，如果 `$(X^TX) $`可逆，那么参数 `$\theta$` 的解唯一。 如果不可逆， 则此时就无法使用正规方程的方法来解。

### 梯度下降 (需要复习)

我们可以采用批量梯度下降算法， 此时有：

`$ \theta_j = \theta_j - \alpha \frac{\delta}{\delta \theta_j} J(\theta) \\ 带入J(\theta) 得： \theta_j = \theta_j - \alpha \frac{1}{m} \sum_{i=1}^m (y^{(i)} - h_\theta(x^{(i)}))x_j^{(i)} \\ 或矩阵表达：\theta_j = \theta_j + \alpha \frac{1}{m}(y-X\theta)^Tx_j $`

### 两种方法的比较 
- 梯度下降中需要选择适当的学习率 `$\alpha $`
- 梯度下降法中需要多次进行迭代，而正规方程只需要使用矩阵运算就可以完成
- 梯度下降算法对多特征适应性较好，能在特征数量很多时仍然工作良好， 而正规方程算法复杂度为 `$O(n^3) $`，所以如果特征维度太高（特别是超过 10000 维），那么不宜再考虑该方法。
- 正规方程中矩阵需要可逆。

---
---
# 岭回归
岭回归本质上是 线性回归 + L2 正则化(regulation)。 `$ \hat{h}{\theta}(x) = h{\theta}(x) + \lambda \sum_i w_i^2 $`

线性回归中通过正规方程得到的 w 的估计： `$ \hat{w} = (X^TX)^{-1}X^Ty $` 但是，当我们有 N 个样本，每个样本有 `$x_i \in R^p$`， 当 N < p 时， `$X^TX$` 不可逆， 无法通过正规方程计算，容易造成过拟合。

岭回归通过在矩阵 `$X^TX$` 上加一个 `$\lambda I$` 来使得矩阵可逆， 此时的 w 的估计： `$ \hat{w} = (X^TX + \lambda I)^{-1}X^Ty $` 而岭回归本质上是对 `$L(w)$` 进行 L2 正则化， 此时的 `$J(w)$` 表示为： 

```math

J(w) = \sum_{i=1}^N ||w^Tx_i - y_i ||^2 + \lambda w^Tw 

= (w^TX^T - Y^T)(Xw - Y) + \lambda w^Tw 

= w^TX^TXw - 2w^TX^TY + Y^TY + \lambda w^Tw 

= w^T(X^TX + \lambda I)w - 2w^TX^TY + Y^TY 

```

那么对 `$w$` 的极大似然估计有： 
```math
\hat{w} = argmax , J(w) \ \frac{\delta J(w)}{\delta w} = 2(X^TX + \lambda I)w - 2 X^TY = 0 
```

那么我们就解得： `$ \hat{w} = (X^TX + \lambda I)^{-1}X^Ty $`

因此说， 岭回归本质上是 线性回归 + L2 正则化， 从而达到抑制过拟合的效果。

---
---
# Lasso
Lasso 回归的本质是 线性回归 + L1 正则化。`$ \hat{h}{\theta}(x) = h{\theta}(x) + \lambda \sum_i |w_i| $`

---
---
# ElasticNet 回归
线性回归 + L1正则化 + L2 正则化
`$ \hat{h}{\theta}(x) = h{\theta}(x) + \lambda \sum_i |w_i|  + \lambda \sum_i w_i^2$`


---
---
# L1 和L2正则化(重点)

相关网页：
https://blog.csdn.net/red_stone1/article/details/80755144

## L2正则化
L2 正则化公式非常简单，直接在原来的损失函数基础上加上权重参数的平方和

```math
loss = E_{in} + \lambda \sum_j \omega_j^2
```
其中，Ein 是未包含正则化项的训练样本误差，λ 是正则化参数

正则化的目的: *限制参数过多或者过大，避免模型更加复杂。* 例如，使用多项式模型，如果使用 10 阶多项式，模型可能过于复杂，容易发生过拟合。所以，为了防止过拟合，我们可以将其高阶部分的权重 w 限制为 0，这样，就相当于从高阶的形式转换为低阶。

为了达到这一目的，最直观的方法就是限制 w 的个数，但是这类条件属于 NP-hard 问题，求解非常困难。所以，一般的做法是寻找更宽松的限定条件：

```math
\sum_j\omega_j^2 \leq C
```
上式是对 w 的平方和做数值上界限定，即所有w 的平方和不超过参数 C。这时候，我们的目标就转换为：最小化训练样本误差 Ein，但是要遵循 w 平方和小于 C 的条件。


## L1正则化

```math
loss = E_{in} + \lambda \sum_j| \omega_j|
```
Ein 优化算法不变，L1 正则化限定了 w 的有效区域是一个正方形，且满足 |w| < C。空间中的点 w 沿着 -∇Ein 的方向移动。但是，w 不能离开红色正方形区域，最多只能位于正方形边缘位置。其推导过程与 L2 类似。

## L1 与 L2 解的稀疏性
L1具有稀疏性，更容易求解


图见链接：
https://blog.csdn.net/red_stone1/article/details/80755144


以二维情况讨论，上图左边是 L2 正则化，右边是 L1 正则化。从另一个方面来看，满足正则化条件，实际上是求解蓝色区域与黄色区域的交点，即同时满足限定条件和 Ein 最小化。对于 L2 来说，限定区域是圆，这样，得到的解 w1 或 w2 为 0 的概率很小，很大概率是非零的。

对于 L1 来说，限定区域是正方形，方形与蓝色区域相交的交点是顶点的概率很大，这从视觉和常识上来看是很容易理解的。也就是说，方形的凸点会更接近 Ein 最优解对应的 wlin 位置，而凸点处必有 w1 或 w2 为 0。这样，得到的解 w1 或 w2 为零的概率就很大了。所以，L1 正则化的解具有稀疏性。

扩展到高维，同样的道理，L2 的限定区域是平滑的，与中心点等距；而 L1 的限定区域是包含凸点的，尖锐的。这些凸点更接近 Ein 的最优解位置，而在这些凸点上，很多 wj 为 0。


## 正则化参数lambda
正则化是结构风险最小化的一种策略实现，能够有效降低过拟合。损失函数实际上包含了两个方面：一个是训练样本误差。一个是正则化项。其中，参数 λ 起到了权衡的作用。

以 L2 为例，若 λ 很小，对应上文中的 C 值就很大。这时候，圆形区域很大，能够让 w 更接近 Ein 最优解的位置。若 λ 近似为 0，相当于圆形区域覆盖了最优解位置，这时候，正则化失效，容易造成过拟合。相反，若 λ 很大，对应上文中的 C 值就很小。这时候，圆形区域很小，w 离 Ein 最优解的位置较远。w 被限制在一个很小的区域内变化，w 普遍较小且接近 0，起到了正则化的效果。但是，λ 过大容易造成欠拟合。欠拟合和过拟合是两种对立的状态。

# LWLR- 局部加权线性回归

在线性回归中，由于最终拟合出来的曲线是一条直线，其拟合能力极为有限（也可以解释为线性回归所求的是具有最小均方误差的无偏估计），因此很容易造成欠拟合现象， 而针对这个问题，有人提出了局部线性回归。
局部线性回归 + 局部加权线性回归


## 局部线性回归：

以一个点x为中心，向前后截取一段长度为frac的数据，对于该段数据用权值函数ww做一个加权的线性回归，记`$(x,\hat{y})$`为该回归线的中心值，其中`$\hat{y}$`为拟合后曲线对应值。对于所有的n个数据点则可以做出n条加权回归线，每条回归线的中心值的连线则为这段数据的Lowess曲线。

核心代码：局部线性回归函数
```
def lwlr(testPoint,xArr,yArr,k=1.0):
    xMat = np.mat(xArr); yMat = np.mat(yArr).T
    m = np.shape(xMat)[0]
    weights = np.mat(np.eye((m)))
    for j in range(m):                      #next 2 lines create weights matrix
        diffMat = testPoint - xMat[j,:]   #difference matrix
        weights[j,j] = np.exp(diffMat*diffMat.T/(-2.0*k**2))   #weighted matrix
    xTx = xMat.T * (weights * xMat)      
    if np.linalg.det(xTx) == 0.0:
        print ("This matrix is singular, cannot do inverse")
        return
    ws = xTx.I * (xMat.T * (weights * yMat))   #normal equation
    return testPoint * w
```




## 局部加权线性回归：
局部加权回归其思想很简单： 我们对一个输入 w 进行预测时，赋予了 x 周围点不同的权值（一般为高斯核权重`$\omega^{(i)}$`），距离 x 越近，权重越高。整个学习过程中误差将会取决于 x 周围的误差，而不是整体的误差，这也就是局部一词的由来。



`$\omega^{(i)} = exp(-\frac{x^{(i)}-x}{2k^2})$`

其损失函数为： `$ loss(\theta) = \frac{1}{2m} \sum_{i=1}^m w^{(i)} (h_\theta(x^{(i)}) - y^{(i)})^2 $`

矩阵表示: `$  loss(\theta) = \frac{1}{2m} (X\theta-y)^TW(X\theta - y) $` 

此时，使用回归方程求得：`$ \theta = (X^TWX)^{-1}X^TWy $`

而通常， `$w^{(i)} $` 服从高斯分布， 在x周围指数型衰减; `$ w^{(i)} = e^{- \frac{|x^{(i)} - x|}{2 k^2 }} $`

k越大权重的差距就越小，k越小权重的差距就很大，仅有局部的点参与进回归系数的求取，其他距离较远的权重都趋近于零。

如果k去进入无穷大，所有的权重都趋近于1，W也就近似等于单位矩阵，局部加权线性回归变成标准的无偏差线性回归，会造成欠拟合的现象

当k很小的时候，距离较远的样本点无法参与回归参数的求取，会造成过拟合的现象。


# 回归树


回归树是可以用于回归的决策树模型，一个回归树对应着输入空间（即特征空间）的一个划分以及在划分单元上的输出值.与分类树不同的是，回归树对输入空间的划分采用一种启发式的方法，会遍历所有输入变量，找到最优的切分变量


## 分类树与回归树
分类树用于分类问题。分类决策树在选取划分点，用信息熵、信息增益、或者信息增益率、或者基尼系数为标准。

Classification tree analysis is when the predicted outcome is the class to which the data belongs.

回归决策树用于处理输出为连续型的数据。回归决策树在选取划分点，就希望划分的两个分支的误差越小越好。

Regression tree analysis is when the predicted outcome can be considered a real number (e.g. the price of a house, or a patient’s length of stay in a hospital)。


## 原理介绍
决策树最直观的理解其实就是，输入特征空间`$R^n$`
，然后对特征空间做划分，每一个划分属于同一类或者对于一个输出的预测值。那么这个算法需要解决的问题是

1. 如何决策边界(划分点)？
2. 尽可能少的比较次数(决策树的形状)

## 最小二乘回归树生成算法
Q1: 选择划分点？

遍历所有的特征(n),对于每一个特征对应`$s_i$`个取值，尝试完所有特征，以及特征所以有划分，选择使得损失函数最小的那组特征以及特征的划分取值。

Q2: 叶节点的输出？

取每个区域所以结果的平均数作为输出

节点的损失函数的形式


```math
min_{j,s}(min_{C1}Loss(y_i,C1)+ min_{C2}Loss(y_i,C2))
```
节点有两条分支，C1是左节点的平均值，C2是右节点的平均值，换句话说，分一次划分都是使得划分出的两个分支的误差和最小。最终得到函数是分段函数


## CART算法
输入： 训练数据集

输出：回归树f ( x ) f(x)f(x)

选择最优的特征j和分切点s

```math
min_{j,s}(min_{C1} \sum_{x_i\in R_1(j,s)} (y_i -c_1)^2  + min_{C2} \sum_{x_i\in R_1(j,s)} (y_i -c_2)^2 ）
```
