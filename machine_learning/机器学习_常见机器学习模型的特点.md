[TOC]

# 回归模型
## 线性回归(Linear Regression)
回归是在建模过程中用于分析变量之间的关系、以及变量是如何影响结果的一种技术。线性回归是指全部由线性变量组成的回归模型。例如，最简单的单变量线性回归(Single Variable Linear Regression)是用来描述单个变量和对应输出结果的关系,可以简单的表示成下面的式子：  因为在实际的建模过程中遇到的问题往往更加复杂，用单个变量不能满足描述输出便变量的关系，所以需要用到更多的变量来表示与输出之间的关系，也就是多变量线性回归(Multi Variable Linear Regression)。多变量线性回归模型如下：  其中a为系数，x是变量，b为偏置。因为这个函数只有线性关系，所以只适用于建模线性可分数据。我们只是使用系数权重来加权每个特征变量的重要性。我们使用随机梯度下降(SGD)来确定这些权重a和偏置b，过程如图所示：
### 线性回归的几个特点：
- 建模速度快，不需要很复杂的计算，在数据量大的情况下依然运行速度很快。
- 可以根据系数给出每个变量的理解和解释
- 对异常值很敏感


## 多项式回归(Polynomial Regression)
线性回归适合于线性可分的数据，当我们处理非线性可分的数据时可以使用多项式回归。在这种回归中，我们是要找到一条曲线来拟合数据点，可以表示成下面的式子：  选择每个变量的确切的质数需要当前数据集合与最终输出的一些先验知识。下面两个图描述了线性回归与多项式回归的比较：

### 多项式回归的特点：
- 能够拟合非线性可分的数据，更加灵活的处理复杂的关系
- 因为需要设置变量的指数，所以它是完全控制要素变量的建模
- 需要一些数据的先验知识才能选择最佳指数
- 如果指数选择不当容易出现过拟合


## 岭回归(Ridge Regression)
分析岭回归之前首先要说的一个共线性(collinearity)的概念，共线性是自变量之间存在近似线性的关系，这种情况下就会对回归分析带来很大的影响。因为回归分析需要我们了解每个变量与输出之间的关系，高共线性就是说自变量间存在某种函数关系，如果两个自变量（X1和X2）之间存在函数关系，那么当X1改变一个单位时，X2也会相应的改变，这样就没办法固定其他条件来对单个变量对输出的影响进行分析了，因为所分析的X1总是混杂了X2的作用，这样就造成了分析误差，所以回归分析时需要排除高共线性的影响。 高共线性的存在可以通过以下几个方式来确定： 1. 尽管从理论上讲，该变量与Y高度相关，但是回归系数却不明显 2. 添加或删除X特征变量时，回归系数会发生明显变化 3. X特征变量具有较高的成对相关性（pairwise correlations）(检查相关矩阵)
标准线性回归的优化函数如下：  其中X表示特征变量，w表示权重，y表示真实情况。岭回归是针对模型中存在的共线性关系的为变量增加一个小的平方偏差因子(也就是正则项)，可以表示成下面的式子：  这样的平方偏差因子向模型中引入了少量偏差，但大大减少了方差。

### 领回归的特点:
- 领回归的假设和最小平方回归相同，但是在最小平方回归的时候我们假设数据服从高斯分布使用的是极大似然估计(MLE)，在领回归的时候由于添加了偏差因子，即w的先验信息，使用的是极大后验估计(MAP)来得到最终的参数
- 没有特征选择功能


## Lasso回归
Lesso与岭回归非常相似，都是在回归优化函数中增加了一个偏置项以减少共线性的影响，从而减少模型方程。不同的是Lasso回归中使用了绝对值偏差作为正则化项，Lasso回归可以表示成下面的式子：  岭回归和Lasso回归之间的差异可以归结为L1正则和L2正则之间的差异： 内置的特征选择(Built-in feature selection):这是L1范数很有用的一个属性，二L2范数不具有这种特性。因为L1范数倾向于产生系数系数。例如，模型中有100个系数，但其中只有10个系数是非零系数，也就是说只有这10个变量是有用的，其他90个都是没有用的。而L2范数产生非稀疏系数，所以没有这种属性。因此可以说Lasso回归做了一种参数选择形式，未被选中的特征变量对整体的权重为0。 稀疏性：指矩阵或向量中只有极少个非零系数。

### L1范数的特点
- L1范数具有产生具有零值或具有很少大系数的非常小值的许多系数的属性。 计算效率：
- L1范数咩有解析解，但L2范数有。这使得L2范数的解可以通过计算得到。
- L1范数的解具有稀疏性，这使得它可以与稀疏算法一起使用，这使得在计算上更有效率。


## 弹性网络回归(ElasticNet Regression)
弹性回归网络是Lesso回归和岭回归技术的混合体。它使用了L1和L2正则化，也达到了两种技术共有的效果，弹性回归网络的表达式如下：  在Lasso和岭回归之间进行权衡的一个实际是运行弹性网络在循环的情况下继承岭回归的一些稳定性。

### 弹性回归网络的优点：
- 鼓励在高度相关变量的情况下的群体效应，而不像Lasso那样将其中一些置为0.当多个特征和另一个特征相关的时候弹性网络非常有用。Lasso倾向于随机选择其中一个，而弹性网络倾向于选择两个。
- 对所选变量的数量没有限制。


# 分类模型
## KNN:
依赖数据，无数学模型可言。适用于可容易解释的模型。 对异常值敏感，容易受到数据不平衡的影响。

## Bayesian:
 基于条件概率， 适用于不同维度之间相关性较小的时候，比较容易解释。也适合增量训练，不必要再重算一遍。应用：垃圾邮件处理。

## Decision Tree:
 此模型更容易理解不同属性对于结果的影响程度（如在第几层)。可以同时处理不同类型的数据。但因为追踪结果只需要改变叶子节点的属性，所以容易受到攻击。应用：其他算法的基石。

## Random Forest:
 随机森林是决策树的随机集成，一定程度上改善了其容易被攻击的弱点。适用于数据维度不太高（几十）又想达到较高准确性的时候。不需要调整太多参数，适合在不知道适用什么方法的时候先用下。

## SVM:
 SVM尽量保持样本间的间距，抗攻击能力强，和RandomForest一样是一个可以首先尝试的方法。

## Logistic regression：
不仅可以输出结果还可以输出其对应的概率。拟合出来的参数可以清晰地看到每一个feature对结果的影响。但是本质上是一个线性分类器，特征之间相关度高时不适用。同时也要注意异常值的的影响。

## Discriminat Analysis：
典型的是LDA，把高维数据投射到低维上，使数据尽可能分离。往往作为一个降维工具使用。但是注意LDA假设数据是正态分布的。

## Neural Network：
准确来说还是一个黑箱，适用于数据量大的时候使用。

## Ensemble-Boosting ：
 每次寻找一个可以解决当前错误的分类器，最后再通过权重加和。好处是自带了特征选择，发现有效的特征。也方便去理解高维数据。

## Ensemble-Bagging:
训练多个弱分类器投票解决。随机选取训练集，避免了过拟合。

## Ensemble-Stacking:
以分类器的结果为输入，再训练一个分类器。一般最后一层用logistic Regression. 有可能过渡拟合，很少使用。

# 其他相关：
## Maximum entropy model
最大熵模型不是一个分类器，是用来判断预测结果好坏的。对于它来说，分类器预测是相当于是：针对样本，给每个类一个出现概率。比如说样本的特征是：性别男。我的分类器可能就给出了下面这样一个概率：高（60%），矮（40%）。 而如果这个样本真的是高的，那我们就得了一个分数60%。最大熵模型的目标就是让这些分数的乘积尽量大。 决策树的数学基础就是他。 LR其实就是使用最大熵模型作为优化目标的一个算法

## Expactation-Maximization
 EM也不是分类器，而是一个思路，很多算法基于此实现。如高斯混合模型，k-means聚类

## Hidden Markov Model
马尔科夫模型不是一个分类器，主要用于通过前面的状态预测后面的状态。主要作用与序列。用于语音识别效果较好。

