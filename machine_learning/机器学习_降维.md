[TOC]

待补充
# PCA与线性判别LDA
## PCA

PCA是比较常见的线性降维方法，通过线性投影将高维数据映射到低维数据中。 所期望的是在投影的维度上，新特征自身的方差尽量大，方差越大特征越有效，尽量使产生的新特征间的相关性越小。

PCA算法的具体操作为对所有的样本进行中心化操作，计算样本的协方差矩阵，然后对协方差矩阵做特征值分解，取最大的n个特征值对应的特征向量构造投影矩阵。

## PCA的算法步骤：
设有m条n维数据。
1）将原始数据按列组成n行m列矩阵X
2）将X的每一行（代表一个属性字段）进行零均值化，即减去这一行的均值
3）求出协方差矩阵
```math
C = 1/m XX^T
```

4）求出协方差矩阵的特征值及对应的特征向量
5）将特征向量按对应特征值大小从上到下按行排列成矩阵，取前k行组成矩阵P
6）Y=PX即为降维到k维后的数据


## LDA
https://www.cnblogs.com/pinard/p/6244265.html

LDA是一种监督学习的降维技术，也就是说它的数据集的每个样本是有类别输出的。这点和PCA不同。PCA是不考虑样本类别输出的无监督降维技术。LDA的思想可以用一句话概括，就是“投影后类内方差最小，类间方差最大”。什么意思呢？

我们要将数据在低维度上进行投影，投影后希望每一种类别数据的投影点尽可能的接近，而不同类别的数据的类别中心之间的距离尽可能的大。