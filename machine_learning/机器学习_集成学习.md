[TOC]

# 集成学习
## Boosting 基于串行策略，各个基分类器之间有依赖。

- 先从初始训练集训练一个弱学习器，初始训练集各个样本权重相同
- 根据上一个弱学习器的表现，调整样本权重，是的分类错误的样本得到更多关注
- 基于调整后的样本分布，训练下一个弱学习器、
- 测试时，对各基学习器加权得到最终结果

## AdaBoost

AdaBoost，是英文"Adaptive Boosting"（自适应增强）的缩写，由Yoav Freund和Robert Schapire在1995年提出。它的自适应在于：前一个基本分类器分错的样本会得到加强，加权后的全体样本再次被用来训练下一个基本分类器。同时，在每一轮中加入一个新的弱分类器，直到达到某个预定的足够小的错误率或达到预先指定的最大迭代次数。

具体说来，整个Adaboost 迭代算法就3步：

- 初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：1/N。
- 训练弱分类器。具体训练过程中，如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。
- 将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。换言之，误差率低的弱分类器在最终分类器中占的权重较大，否则较小。


## GBDT
Gradient Boosting Decision Tree， 叫 梯度提升决策树。则与AdaBoost不同，GBDT每一次的计算是都为了减少上一次的残差，进而在残差减少**（负梯度）**的方向上建立一个新的模型。

GBDT的原理很简单，就是所有弱分类器的结果相加等于预测值，然后下一个弱分类器去拟合误差函数对预测值的梯度/残差(这个梯度/残差就是预测值与真实值之间的误差)。当然了，它里面的弱分类器的表现形式就是各棵树。所以最终结果Y = Y1 + Y2 + Y3。

回归任务下，GBDT 在每一轮的迭代时对每个样本都会有一个预测值，此时的损失函数为均方差损失函数，

```math
l(y_i,y_i') = \frac{1}{2}(y_i -y_i')^2
```
那此时的负梯度是这样计算的

```math
-[\frac{\partial l( y_i,y_i')}{\partial y_i'}]

= (y_i -y_i')
```

所以，当损失函数选用均方损失函数是时，每一次拟合的值就是（真实值 - 当前模型预测的值），即残差。此时的变量是，即“当前预测模型的值”，也就是对它求负梯度。


## XGBoost(需要复习)
https://blog.csdn.net/v_JULY_v/article/details/81410574

eXtreme Gradient Boosting，本质上还是GBDT

### 主要创新点：
- 设计和构建高度可扩展的端到端提升树系统。
- 提出了一个理论上合理的加权分位数略图（weighted quantile sketch ）来计算候选集。
- 引入了一种新颖的稀疏感知算法用于并行树学习。 令缺失值有默认方向。
- 提出了一个有效的用于核外树形学习的缓存感知块结构。 用缓存加速寻找排序后被打乱的索引的列数据的过程。

XGBoost是一个树集成模型，他将K（树的个数）个树的结果进行求和，作为最终的预测值。
