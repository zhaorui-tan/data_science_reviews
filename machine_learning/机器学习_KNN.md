[TOC]

# KNN
kMN本质上没有学习过程，所以单独拿出来。

KNN算法是分类算法的一种，也属于监督学习算法，其基本思想为：

当输入一个新的样本时，将新数据的每个特征与样本集中每个样本数据的特征进行比较。

从样本集中选取最相近的 K 个样本，然后依据某种决策原则（少数服从多数）来判定这个新样本的 label。

在KNN算法中，有三个主要要素：距离度量，K 的取值，分类决策规则。

## 算法步骤
- 计算新数据与数据集中所有样本之间的距离， 距离度量公式可以选择多种，如欧式距离，曼哈顿距离等。
- 按照距离，将样本按照距离值进行排序
- 选取与当前数据距离最小的 K 个点。
- 返回这 K 个点的 label， 依据某种决策原则来判定这个新数据的 label 。

## K 的选择
如果 K 较小，预测结果会对邻近的点十分敏感，如果邻近点恰好是噪声，则很容易预测错误。 换个角度说说， K的减小意味着模型变得复杂，容易发生过拟合。

如果K 较大， 此时不相似的样本也会对预测产生作用，使得预测发生错误。换个角度说，K的增大意味着模型变得简单，容易发生欠拟合。

# KNN的优化
- 通过kd-tree或者ball-tree提高搜索效率.
- 通过局部敏感哈希（LSH）来计算近似最临近。
    - annoy或者flann