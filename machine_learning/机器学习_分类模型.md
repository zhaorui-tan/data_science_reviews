[TOC]

# 逻辑回归

## 简介
logistic回归用于解决的是分类问题，其基本思想是：根据现有数据对分类边界线建立回归公式,以此进行分类。也就是说，logistic 回归不是对所有数据点进行拟合，而是要对数据之间的分界线进行拟合。

逻辑回归的本质： 极大似然估计

逻辑回归的激活函数：Sigmoid

逻辑回归的代价函数：交叉熵（H(x)+H(Y) - H(X,Y)）


## Logistic 回归的数学表达
```math
h_\theta(x) = sigmoid(\theta^T X) = \frac{1}{1 + e^{-\theta^T X}} 
```
## 如何求解最优的 `$\theta$`
首先，我们依旧是要找到一个合适的损失函数，在Logistic回归中的损失函数为： 
```math
Cost(h_{\theta}(x),y) =
\begin{cases} -log(h_{\theta(x)}) & if , y = 1\\ -log(1-h_{\theta(x)}) & if , y = 0
\end{cases} 
```

```math
J(\theta) = - \frac{1}{m} \left[ \sum_{i=1}^m y^{(i)}log(h_\theta(x^{(i)})) + (1-y^{(i)}) log(1 - h_\theta(x^{(i)})) \right] 
```

我们最终给它加一个正则化项：

```math
J(\theta) = - \frac{1}{m} \left[ \sum_{i=1}^m y^{(i)}log(h_\theta(x^{(i)})) + (1-y^{(i)}) log(1 - h_\theta(x^{(i)})) \right] + \frac{\lambda}{2m} \sum_{j=1}^{m}\theta_j^2 
```

最后，我们要求最优参数的话，依旧是使用梯度下降算法来获取`$J(\theta)$` 的最小值时对应的参数。


## 推导

sigmoid : `$ g(z) = \frac{1}{1+e^{-z}} \\ g'(z) = g(z)(1-g(z)) $`

LR 的定义： `$ h_{\theta}(x) = g(\theta^Tx) = \frac{1}{1 + e^{-\theta^Tx}} $`

LR 满足伯努利分布： `$ P(Y=1|x; \theta) = h_{\theta}(x) \ P(Y=0|x; \theta) = 1 - h_{\theta}(x) \ p(y|x; \theta) = (h_{\theta}(x))^y (1-h_{\theta}(x))^{1-y} $`

损失函数（极大似然）: 对于训练数据集，特征数据 `$x={x_1, ...x_m}$` 和其对应的分类标签 `$y = {y_1,...y_m}$` ， 假设 m 个样本是相互独立的，那么极大似然函数为：

```math


L(\theta) = \prod_{i=1}^m p(y^{(i)}|x(i);\theta) 

= \prod_{i=1}^m (h_{\theta}(x^{(i)}))^{y^{(i)}} (1-h_{\theta}(x^{(i)}))^{1-y^{(i)}}
```
那么它的 log 似然为： 

```math
L(\theta) = log L(\theta ) 

= \sum_{i=1}^m y^{(i)} log h(x^{(i)}) + (1-y^{(i)}) log (1-h(x^{(i)}))
```

参数优化（梯度上升） 


```math
\frac{\partial L(\theta)}{\partial \theta_j} = (y \frac{1}{g(\theta^Tx)} - (1-y) \frac{1}{1 -g(\theta^Tx)}) \frac{\delta g(\theta^Tx)}{\delta \theta_j} 

= (y \frac{1}{g(\theta^Tx)} - (1-y)\frac{1}{1 -g(\theta^Tx)} ) g(\theta^Tx)(1-g(\theta^Tx)) \frac{\delta \theta^Tx}{\theta_j} 

= (y (1 - g(\theta^Tx)) - (1-y) g(\theta^Tx)) x_j 

= [y - h_{\theta} (x)]x_j


\theta_j = \theta_j + \alpha \frac{\partial L(\theta)}{\partial \theta} 

= \theta_j + \alpha [y^{(i)} - h_{\theta} (x^{(i)})]x_j^{(i)}

```

损失函数： 

```math
J(\theta) = 

-\frac{1}{m} \left[ \sum_{i=1}^m y^{(i)}log(h_\theta(x^{(i)})) + (1-y^{(i)}) log(1 - h_\theta(x^{(i)})) \right] 
```


## LR 如何实现多分类？
方式1： 修改逻辑回归的损失函数，使用softmax函数构造模型解决多分类问题，softmax分类模型会有相同于类别数的输出，输出的值为对于样本属于各个类别的概率，最后对于样本进行预测的类型为概率值最高的那个类别。softmax 相当于逻辑回归的一般形式

方式2： 根据每个类别都建立一个二分类器，本类别的样本标签定义为0，其它分类样本标签定义为1，则有多少个类别就构造多少个逻辑回归分类器。
若所有类别之间有明显的互斥则使用softmax分类器，若所有类别不互斥有交叉的情况则构造相应类别个数的逻辑回归分类器。

## LR 为何要对特征进行离散化
### 非线性

逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合； 离散特征的增加和减少都很容易，易于模型的快速迭代；

### 速度快

稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展

### 鲁棒性

离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；

### 方便交叉与特征组合：

离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力。

### 稳定性：

特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问。
简化模型： 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。

## 逻辑回归中，增大 L1 正则化会是什么结果

所有参数 w 都会变成 0。

---
---
---
---
---

# 决策树


算法    |   划分标准
     ---|---
ID3     |   信息增益
C4.5    |   信息增益率
CART    |	基尼系数

## 简介
决策树是一个分而治之的递归过程。

开始，构建根节点，将所有训练数据都放在根节点。
然后，选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。
如果子集未分类完毕，则在子集中选择一个最优特征，继续进行划分，直到所有训练数据子集都被正确分类或没有合适的特征为止。

## 决策树三要素

**特征选择：**
从训练数据中众多的特征中选择一个特征作为当前节点的分裂标准，如何选择特征有着很多不同量化评估标准标准，从而衍生出不同的决策树算法。

**决策树生成：** 根据选择的特征评估标准，从上至下递归地生成子节点，直到数据集不可分则停止决策树停止生长。

**决策树的修剪：** 决策树容易过拟合，一般来需要剪枝，缩小树结构规模、缓解过拟合。剪枝技术有预剪枝和后剪枝两种。

## 特征的选择
有三种方法进行特征选择：ID3: 信息增益，C4.5: 信息增益比，CART: 基尼系数

### 1. ID3
思想： 计算所有特征划分数据集D，得到多个特征划分数据集D的信息增益，从这些信息增益中选择最大的，因而当前结点的划分特征便是使信息增益最大的划分所使用的特征。

1. 对当前例子集合，计算各属性的信息增益；
2. 选择信息增益最大的属性Ak；
3. 把在Ak处取值相同的例子归于同一子集，Ak取几个值就得几个子集；
4. 对既含正例又含反例的子集，递归调用建树算法；
5. 若子集仅含正例或反例，对应分枝标上P或N，返回调用处。

信息增益： 度量以某特征划分数据集前后的信息熵的差值。 信息熵能够表示样本集合的不确定性，因此我们能够通过前后集合信息熵的差值来衡量使用当前特征对于样本集合D划分效果的好坏。

假设划分前样本集合D的熵为 `$H(D)$`。使用某个特征A划分数据集D，计算划分后的数据子集的熵为 `$H(D|A)$` 。 

信息熵：

```math
H(D) = - \sum_{k=1}^k \frac{|C_k|}{|D|} log_2 \frac{|C_k|}{|D|}
```

条件熵： 

```math
H(D|A) = \sum_{i=1}^n \frac{|D_i|}{|D|} H(D_i) 
```

信息增益：

```math
g(D,A)=H(D)-H(D|A) 
```

注意： 在决策树构建中，我们总是希望集合往最快到达纯度更高的子集合发展，因此我们总是选择是的信息增益最大的特征来划分当前集合。

缺点：信息增益对分支较多的属性有所偏好，因此有人提出采用信息增益比来划分特征。

### 2. C4.5
**信息增益比本质：** 在信息增益的基础之上乘上一个惩罚参数。特征个数较多时，惩罚参数较小；特征个数较少时，惩罚参数较大。 

```math
信息增益比 = 惩罚参数 \times 信息增益 \\信息增益比：g_R(D,A) = \frac{g(D,A)}{H(D)} 
```

信息增益比对可取值数目较少的属性有所偏好。C4.5 先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择信息增益比最高的。

相对于 ID3 算法的改进：用信息增益比来选择属性，克服了用信息增益选择属性偏向选择多值属性的不足

### 3. CART
CART 既可以用于分类，也可以用于回归。

基尼系数 `$Gini(D)$` 表示集合 D 的不确定性，基尼系数 `$Gini(D, A=a)$` 表示集合 D 经过 A = a 分割后的不确定性。 基尼系数越小，样本的不确定性越小。分类问题中，假设有 K 个类，样本点属于第 k 类的概率为 `$p_k$`，则概率分布的基尼系数定义为 

```math
Gini(D) = \sum_{k=1}^{K}p_k(1-p_k)

= 1 - \sum_{k=1}^{K} p_k^2 
``` 

如果样本集合 `$D$ `根据特征 `$A$` 是否取一可能值 `$a$` 被分割成 `$D_1$` 和 `$D_2$` 两部分， 那么在特征 A 的条件下， 集合 D 的基尼系数定义为：

```math
Gini(D, A=a) = \frac{D_1}{D}Gini(D_1) + \frac{D_2}{D} Gini(D_2) 
``` 

从根节点开始，对节点计算现有特征的基尼系数，对于每一个特征，根据 “是” 与 “否” 划分为两个部分，根据上式计算划分过后的基尼系数。

在所有可能的特征 A 以及该特征所有的可能取值a中，选择基尼指数最小的特征及其对应的取值作为最优特征和最优切分点。然后根据最优特征和最优切分点，将本节点的数据集二分，生成两个子节点。

对两个字节点递归地调用上述步骤，直至节点中的样本个数小于阈值，或者样本集的基尼指数小于阈值，或者没有更多特征后停止。


## 剪枝处理
1. 剪枝的作用

剪枝处理是决策树学习算法用来解决过拟合的一种办法。
在决策树算法中，为了尽可能正确分类训练样本， 节点划分过程不断重复， 有时候会造成决策树分支过多，以至于将训练样本集自身特点当作泛化特点， 而导致过拟合。 因此可以采用剪枝处理来去掉一些分支来降低过拟合的风险。

2. 预剪枝

在决策树生成过程中，在每个节点划分前先估计其划分后的泛化性能， 如果不能提升，则停止划分，将当前节点标记为叶结点。

3. 后剪枝

生成决策树以后，再自下而上对非叶结点进行考察， 若将此节点标记为叶结点可以带来泛化性能提升，则修改之。


---
---
---
---
---

# 感知机

感知机是1957年，由Rosenblatt提出会，是神经网络和支持向量机的基础。


感知机可以拟合任何线性函数，这也就意味着任何线性分类或线性回归问题都可以用感知器来解决。



```math
f(x) = sign(w^Tx+b）

= \begin{cases} +1, & a \ge 0 \\ -1, & a < 0 \end{cases} 
```

感知器的训练主要包含两大问题：
- 如何判断一个样本是不是被正确分类？
- 如何定义损失函数？

##  如何判断一个点被正确分类？
误分类分为以下两种情况：
- 当`$w^T x_i > 0 $` 时，得出`$y_i = -1 $` ， 此时出现误分类
- 当`$w^Tx_i < 0 $` 时，得出`$ y_i = 1$` ，此时也出现误分类

那么，我们分析，对于误分类的数据`$(x_i,y_i) $` 来说，必有： 
`$y_iw^T x_i > 0 $` 那么，我们得出，我们可以通过 `$- y_iw^Tx_i > 0$` 来得出一个点是不是误分类点。

## 损失函数
感知机中采用的损失函数是误分类点到超平面S的总距离。

输入样本中的任意一点到超平面S的距离： `$ L = \frac{1}{||w||}|w^T x_i| , \quad ||w||是w的L_2范数 $`

那么误分类点到超平面 S 的距离为： 


```math
\frac{1}{||w||}y_i(w x_i + b)


```

M集合是误分类点的集合。

`$ \frac{1}{||w||}$`可以不用考虑，这样我们也可以得出我们的损失函数如下：
```math
L(w,b) = - \frac{1}{||\omega||}\sum_{x_i \in M }{y_i(w x_i + b) } 

可简化为

L(w,b) = - \sum_{x_i \in M }{y_i(w x_i + b)} 
```
## 为什么可以不考虑||𝑤||


网上有人说1||𝑤||是个定值，但是个人觉得平面不唯一，这个值肯定也会变。通过参考他人观点结合思考，觉得原因可以列为以下两点。

1/||𝑤||不影响𝑦𝑖(𝑤⋅𝑥𝑖+𝑏)正负的判断，即不影响学习算法的中间过程。因为感知机学习算法是误分类驱动的，这里需要注意的是所谓的“误分类驱动”指的是我们只需要判断−𝑦𝑖(𝑤⋅𝑥𝑖+𝑏)的正负来判断分类的正确与否，而1/||𝑤||并不影响正负值的判断。所以1/||𝑤||对感知机学习算法的中间过程可以不考虑。

1/||𝑤||不影响感知机学习算法的最终结果。因为感知机学习算法最终的终止条件是所有的输入都被正确分类，即不存在误分类的点。则此时损失函数为0. 对应于
```math
-\frac{1}{||w||}\sum_{i\in M } y_i(w x_i + b)

```
即分子为0.则可以看出/1||𝑤||对最终结果也无影响。
综上所述，即使忽略1/||𝑤||，也不会对感知机学习算法的执行过程产生任何影响。反而还能简化运算，提高算法执行效率。

## 感知机算法
感知机学习算法是对上述损失函数进行极小化，求得𝑤和𝑏。但是用普通的基于所有样本的梯度和的均值的批量梯度下降法（BGD）是行不通的，原因在于我们的损失函数里面有限定，只有误分类的M集合里面的样本才能参与损失函数的优化。所以我们不能用最普通的批量梯度下降,只能采用随机梯度下降（SGD）。目标函数如下：

```math
L(w,b)=arg\min_{w,b}(-\sum\limits_{{{x}_{i}}\in{M}}{{{y}_{i}}(w\cdot {{x}_{i}}+b)})
```




## 求取w， b
损失函数对于`$w$` 方向上的梯度为： 
```math
\frac{\partial {L}}{\partial{w}} = - \sum_{x_i \in M} y_i x_i 
```

那么随便给定一个误分类点`$(x_i, y_i)$`， 对w，b的更新如下： `$ w \leftarrow w + \eta y_i x_i $` 

---
---
---
---
---

# 朴素贝叶斯

## 1. 基本概念
1. 条件概率
`$ P(X|Y) = \frac{P(X,Y)}{P(Y)} $`

`$P(X|Y)$`含义： 表示 y 发生的条件下 x 发生的概率。

2. 先验概率

含义： **表示事件发生前的预判概率。** 这个可以是基于历史数据统计，也可以由背景常识得出，也可以是主观观点得出。一般都是单独事件发生的概率，如 P(A)
 
3. 后验概率

基于先验概率求得的反向条件概率，形式上与条件概率相同（若 P(X|Y) 为正向，则 P(Y|X) 为反向）

## 2. 贝叶斯公式
贝叶斯公式如下： `$ P(Y|X) = \frac{P(X|Y) P(Y)}{P(X)} \ $`

P(Y) 叫做先验概率，意思是事件X发生之前，我们对事件Y发生的一个概率的判断

P(Y|X) 叫做后验概率，意思是时间X发生之后，我们对事件Y发生的一个概率的重新评估

P(Y,X) 叫做联合概率， 意思是事件X与事件Y同时发生的概率。

## 3. 条件独立假设
```math
P(x|c) = p(x_1, x_2, \cdots x_n | c) = \prod_{i=1}^Np(x_i | c)
```

朴素贝叶斯采用条件独立假设的动机在于： 简化运算。


## 朴素贝叶斯中的三种模型

https://www.cnblogs.com/lovewhale1997/p/11265029.html

**1. 多项式模型**

多项式模型适用于离散特征情况，在文本领域应用广泛， 其基本思想是：我们将重复的词语视为其出现多次。



该模型常用于文本分类，特征是单词，值是单词的出现次数。

在多项式模型中，设某文档d={t1,t2,...,tk}，ti(i=1,2,...,k)为在该文档d中出现的单词，允许重复。

**则先验概率p(c) = 类c下单词总数 / 整个训练样本的单词总数**

**类条件概率 p(t_k|c) = (类c下单词tk在各个文档出现的数量之和+1) / (类c下单词总数 + |V|)**

V是训练样本中所有单词的集合(set，即每个单词有且仅能出现一次)，即该训练样本的词汇表。

在这里解释一下为何分子要加1，分母加|V|：

我们已知朴素贝叶斯的“朴素点”在于假设每个特征之间相互独立，在本例中就是任何单词之间相互独立，若在输入某个文档做分类时，发现该文档中的某个单词在词汇表中没有出现过，就会出现p(tk|c)=0，最终导致后验概率为0，如果该文档是一篇垃圾文档，将会被模型分类成有用文档，结果变得不合理了。拉普拉斯平滑 **(Laplace Smoothing)** 又被称为加1平滑，被用来解决零概率问题。拉普拉斯平滑就是在计算类条件概率时分子加1，分母加可取变量的个数（本例中为词汇表中单词的数量）。




**2. 高斯模型**

https://blog.csdn.net/u012162613/article/details/48323777

http://www.letiantian.me/2014-10-12-three-models-of-naive-nayes/


当特征是连续变量的时候，运用多项式模型就会导致很多（不做平滑的情况下），此时即使做平滑，所得到的条件概率也难以描述真实情况。所以处理连续的特征变量，应该采用高斯模型。

高斯模型适合连续特征情况， 我们先给出高斯公式：
```math
P(x_{i}|y_{k}) = \frac{1}{\sqrt{2\pi\sigma_{y_{k}}^{2}}}exp( -\frac{(x_{i}-\mu_{y_{k}})^2} {2\sigma_{y_{k}}^{2}} ) 
```

比如：
类身体特征的统计资料： 可以看到每个特征都为连续值，并非离散值。

此时可以假设男性和女性的身高、体重和脚掌都是正态分布，通过样本分别计算出均值μ和方差σ^2。

比如男性的身高是均值为5.94，方差为0.035的正态分布，此时有一个人身高为6英尺，则:

据此可以求出其他特征的类条件概率，从而最终推断出该人是男性还是女性。


**3. 伯努利模型**

在伯努利模型中，每个特征的取值是布尔型的，即true和false，或者1和0。在文本分类中，就是一个特征有没有在一个文档中出现。

**先验概率p(c)=类c下文档总数/整个训练样本的文档总数**

**类条件概率p(t_k|c)=类c下包含单词tk的文档总数 + 1 /类c下的文档总数+2**

伯努利模型适用于离散特征情况，它将重复的词语都视为只出现一次。 `$ P( " 代开“， ”发票“， ”发票“， ”我“ | S) = P("代开" | S) P( ”发票“ | S) P("我" | S) $` 我们看到，”发票“出现了两次，但是我们只将其算作一次。

---
---
---

# SVM支持向量机

## 简介
https://zhuanlan.zhihu.com/p/77750026

SVM 三宝： 间隔，对偶，核技巧。它属于判别模型。Support Vector Machine

**支持向量：** 在求解的过程中，会发现只根据部分数据就可以确定分类器，这些数据称为支持向量。

支持向量机（SVM）：其含义是通过支持向量运算的分类器。

SVM 是一种二分类模型， 它的目的是寻找一个超平面来对样本进行分割，分割的依据是间隔最大化，最终转化为一个凸二次规划问题来求解。


## 基础知识

#### 1. 线性可分

`$D_0$` 和` $D_1$` 是 n 维空间中的两个点集， 如果存在 n 维向量 `$w$ `和实数 `$b$ `， 使得：

```math
wx_i +b > 0; \quad x_i \in D_0 \\ wx_j + b < 0; \quad x_j \in D_1 
```

则称 `$D_0$` 与 `$D_1$` 线性可分。

#### 2. 最大间隔超平面

能够将 `$D_0$` 与 `$D_1$` 完全正确分开的 `$wx+b = 0$` 就成了一个超平面。

为了使得这个超平面更具鲁棒性，我们会去找最佳超平面，以最大间隔把两类样本分开的超平面，也称之为最大间隔超平面。

两类样本分别分割在该超平面的两侧
两侧距离超平面最近的样本点到超平面的距离被最大化了

#### 3. 什么是支持向量？

训练数据集中与分离超平面距离最近的样本点成为支持向量

#### 4. SVM 能解决哪些问题？

**线性分类：** 对于n维数据，SVM 的目标是找到一个 n-1 维的最佳超平面来将数据分成两部分。

通过增加一个约束条件： 要求这个超平面到每边最近数据点的距离是最大的。

非线性分类： SVM通过结合使用拉格朗日乘子法和KTT条件，以及核函数可以生产线性分类器

#### 5. 支持向量机的分类

- 硬间隔SVM（线性可分SVM)： 当训练数据可分时，通过间隔最大化，学习一个线性表分类器。
- 软间隔SVM(线性SVM)：当训练数据接近线性可分时，通过软间隔最大化，学习一个线性分类器。
- Kernel SVM： 当训练数据线性不可分时，通过使用核技巧及软间隔最大化，学习非线性SVM。

---
## **必会：硬间隔最大化 --> 学习的对偶问题 --> 软间隔最大化 --> 非线性支持向量机（核技巧）**
---

## 硬间隔 SVM

任意分离超平面可定义为： `$ w^Tx + b = 0 $` 二维空间中点 `$(x,y)$` 到直线 `$Ax + By + C=0$` 的距离公式为： 
```math
\frac{|Ax+ By + C|}{\sqrt{A^2 + B^2}} 
```

扩展到n维空间中，任意点 `$x$` 到超平面`$w^Tx + b = 0$ `的距离为：
```math
\frac{|w^Tx + b|}{||w||} 

||w|| = \sqrt{w_1^2 + ... + w_n^2} 
```
假设，支持向量到超平面的距离为 `$d$` ，那么就有： 

```math
\begin{cases} \frac{w^Tx_i + b}{||w||} \geq d, & y_i = +1 \\ \frac{w^Tx_i + b}{||w||} \leq -d, & y_i = -1 \end{cases} 
```

稍作转化可得到： 
```math
\begin{cases} 
\frac{w^Tx_i + b}{||w||d} \geq 1, & y_i = +1 \\ \frac{w^Tx_i + b}{||w||d} \leq -1, & y_i = -1 \end{cases}
```
 

考虑到 `$||w||d$` 为正数，我们暂且令它为 1（之所以令它等于 1，是为了方便推导和优化，且这样做对目标函数的优化没有影响）： 

```math
\begin{cases} w^Tx_i + b >= +1, & y_i = +1 \\ w^Tx_i + b <= -1, & y_i = -1 \end{cases}
```


两个方程合并，则有：

```math
y_i (w^Tx_i + b) \geq 1
```
那么我们就得到了最大间隔超平面的上下两个超平面：


两个异类超平面的公式分别为： 
```math
\begin{cases} w^Tx_i + b = +1, & y_i = +1 \\ w^Tx_i + b = -1, & y_i = -1 \end{cases}
```
 
 那么两个异类超平面之间的间隔为： 
```math
\frac{2}{||w||}
```
 我们的目的是最大化这种间隔： 

```math
max \quad \frac{2}{||w||} 

:= min \quad \frac{1}{2} ||w|| 

:= min \quad \frac{1}{2} ||w||^2
```

 
 那么我们的最优化问题为： 
 
```math
min \quad \frac{1}{2} ||w||^2 

使得 y_i(w^Tx_i + b) \geq 1
```

## 对偶问题

拉格朗日乘数法的基本形态`$z = f(x,y)$`在满足`$g(x,y) = 0$`条件下的极值，可以转化为函数`$F(x,y,\lambda) = f(x,y) + \lambda g(x,y)$`

求函数在满足下的条件极值，可以转化为函数的无条件极值问题。
 　　
### 1. 拉格朗日乘数法 - 等式约束优化问题

高等数学中，其等式约束优化问题为： 


```math
min , f(x_1, ..., x_n) \quad st. \quad h_k(x_1, ... , x_n) = 0

那么令： 
L(x, \lambda) = f(x) + \sum_{k=1}^l \lambda_k h_k(x)
```


`$L(x, \lambda)$` ： Lagrange 函数

`$\lambda$ `： Lagrange 乘子，没有非负要求

利用必要条件找到可能的极值点，我们得到如下的方程组： 
```math
\begin{cases} \frac{\delta L}{ \delta x_i} = 0, & i=1,2,...,n \\\frac{\delta L}{ \delta \lambda_k} = 0, & k=1,2,...,l \end{cases}
```
 等式约束下的Lagrange 乘数法引入了 `$l$` 个 Lagrange 乘子，我们将 `$x_i$`与 `$\lambda_k$` 一视同仁，将`$\lambda_k$` 也看做优化变量，那么共有 `$(n+l)$` 个优化变量。

### 2. 拉格朗日乘数法 - 不等式约束优化问题

对于不等式约束优化问题，其主要思想在于将不等式约束条件转变为等式约束条件，引入松弛变量，将松弛变量也是为优化变量。

对于我们的问题： 
```math
min \quad \frac{1}{2} ||w||^2 \quad \ st. \quad g_i(w) = 1- y_i(w^Tx_i + b) \leq 0  

引入松弛变量 a_i^2 得到： 

f(w) = \frac{1}{2} ||w||^2 

g_i(w) = 1- y_i(w^Tx_i + b) 

h_i(w, a_i) = g_i(w) + a_i^2 = 0 
```

这里加平方主要为了不再引入新的约束条件，如果只引入 `$a_i$` 那我们必须要保证 `$a_i \geq 0$` 才能保证 `$h_i(w, a_i)$` ，这不符合我们的意愿。

此时，我们就将不等式约束转化为等式约束，并得到 Lagrange 函数： 


```math
 L(w, \lambda, a) 
 
 =  f(w) + \sum_{i=1}^n \lambda_i h_i(w) 
 
 = f(w) + \sum_{i=1}^n \lambda_i [g_i(w) + a_i^2] \quad \lambda_i \geq 0 
```


那么我们得到方程组有： 


```math
\begin{cases} \frac{\delta L}{ \delta w_i} =\frac{\delta f}{\delta w_i} + \sum_{i=1}^n \lambda_i  & \frac{\delta g_i}{\delta w_i} =0 \\ \frac{\delta L}{ \delta a_i} = 2 \lambda_i a_i =0 \\ \frac{\delta L}{ \delta \lambda_i}=g_i(w) + a_i^2 = 0 \ & \lambda_i \geq 0 \qquad ？ 需要看一看 \end{cases} 
```


针对 `$\lambda_i a_i = 0$` 有两种情况：

`$\lambda_i = 0, a_i \neq 0$`：此时约束条件 `$g_i(w)$` 不起作用且 `$g_i(w) < 0$`

`$\lambda_i \neq 0, a_i = 0$`： 此时 `$g_i(w)=0, \lambda_i > 0$`， 可以理解为约束条件 `$g_i(w)$ `起作用了， 且`$g_i(w) = 0$`

综合可得：`$\lambda_ig_i(w) = 0$`， 且在约束条件起作用时` $\lambda_i > 0, g_i(w) = 0 $`； 约束不起作用时， `$\lambda_i = 0, g_i(w) < 0 $`。

此时，方程组转化为： 


```math
\begin{cases} \frac{\delta L}{ \delta w_i} =\frac{\delta f}{\delta w_i} + \sum_{i=j}^n \lambda_j & \frac{\delta g_i}{\delta w_i} =0 \\ \lambda_ig_i(w) = 0 & \ g_i(w) \leq 0 \  \lambda_i \geq 0 \end{cases}
```


以上便是不等式约束优化优化问题的 KKT(Karush-Kuhn-Tucker) 条件， `$\lambda_i$` 称为 KKT 乘子。

KTT 条件中，对于不同样本点来说

支持向量 `$g_i(w) = 0$`， 此时 `$\lambda_i > 0$` 即可

其余向量 `$g_i(w) < 0$`， 此时 `$\lambda_i = 0$`

回到原优化问题中： 


```math
L(w, \lambda, a) =  f(w) + \sum_{i=1}^n \lambda_i h_i(w) 

= f(w) + \sum_{i=1}^n \lambda_i [g_i(w) + a_i^2] 

= f(w) + \sum_{i=1}^n \lambda_i g_i(w) + \sum_{i=1}^n \lambda_i a_i^2
```


由于 `$\sum_{i=1}^n \lambda_ia_i^2 \geq 0$`， 

那么问题可以转化为： 


```math
L(w, \lambda) =  f(w) + \sum_{i=1}^n \lambda_i g_i(w) 

= \frac{1}{2} ||w||^2 + \sum_{i=1}^n \lambda_i (1- y_i(w^Tx_i + b) )
```


假设我们找到了最佳的参数 `$w$` 使得 `$\frac{1}{2} ||w||^2 = p$` ，又因为 `$\sum_{i=1}^n \lambda_i (1- y_i(w^Tx + b) ) \leq 0$`， 因此有 `$L(w, \lambda) \leq p$`， 我们需要找到最佳的参数 `$\lambda$`， 使得 `$L(w, \lambda)$` 接近 p， 此时问题转化为： 


```math
min_wmax_{\lambda} , L(w, \lambda) \quad \ s.t. \quad \lambda_i \geq 0
```

## 3. 引入对偶问题 -- TODO
弱对偶性： 最大的里面挑出来的最小的也要比最小的里面挑出来的最大的要大 `$ min , max , f \geq max , min , f $`

强对偶性：KKT 条件是强对偶性的充要条件。

KKT详解：https://www.cnblogs.com/liaohuiqiang/p/7805954.html


## SVM 优化
SVM 的优化问题为： `$ min \quad \frac{1}{2} ||w||^2 \quad \ st. \quad g_i(w) = 1- y_i(w^Tx_i + b) \leq 0 $`

构造拉格朗日函数： `$ min_{w,b}max_{\lambda} L(w, b, \lambda) = \frac{1}{2} ||w||^2 + \sum_{i=1}^n \lambda_i (1- y_i(w^Tx_i + b) ) \ s.t. \lambda_i \geq 0 $`

利用强对偶性转化： `$ max_{\lambda}min_{w,b} , L(w, b, \lambda) $` 对参数 `$w, b$` 求偏导有：

```math
\frac{\delta L}{\delta w} = w - \sum_{i=1}^n \lambda_i x_i y_i = 0 

\frac{\delta L}{\delta b} = \sum_{i=1}^n \lambda_i y_i = 0
```

得到： 
```math
 w = \sum_{i=1}^n \lambda_i x_i y_i 
 
 \sum_{i=1}^n \lambda_i y_i = 0 
```
 将两式带入到 `$L(w, b, \lambda)$` 中有： 

```math
min_{w,b} , L(w, b, \lambda) =\sum_{j=1}^n \lambda_i - \frac{1}{2} \sum_{i=1}^n \sum_{j=1}^n \lambda_i \lambda_j y_i y_j x_i^Tx_j
```


求解模型：

## 软间隔SVM
软间隔允许部分样本点不满足约束条件：
```math
 1 - y_i (w^Tx_i + b) \leq 0 
```

## Kernel SVM
### 1. 思想
对于在有限维度向量空间中线性不可分的样本，我们将其映射到更高维度的向量空间里，再通过间隔最大化的方式，学习得到支持向量机，就是非线性 SVM。

用 x 表示原来的样本点，用 `$\phi(x)$` 表示 x 映射到新特征空间后的新向量。那么分割超平面可以表示为： 

```math
f(x) = w \phi (x) + b
```

此时，非线性 SVM 的对偶问题转化为： 

```math
min_{\lambda} [\frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^n \lambda_i \lambda_j y_i y_j \phi^T(x_i) \phi(x_j)]
```
其中 `$\phi^T(x_i) 和 \phi(x_j)$` 是`$x_i 和x_j$`的高维映射
当映射维度过高时，`$\phi^T(x_i)\phi(x_j)$`很难求解，因此引入核函数



### 2. 核函数的作用
原理： cover's theorem，数据在高维空间里更线性可分

目的： 将原坐标系中线性不可分数据通过核函数映射到另一空间，尽量使数据在新的空间里线性可分。

如果找到一个函数`$ K(x_i, x_j) $`，使得`$ K(x_i, x_j)  = \phi^T(x_i)\phi(x_j) $`，我们称其为核函数

通过核函数我们可以避免计算`$\phi^T(x_i)和\phi(x_j)$`从而直接求解`$\phi^T(x_i)\phi(x_j)$`


### 3. 常见核函数

https://www.cnblogs.com/infinite-h/p/10723853.html

#### 线性核函数

```math
K(x_i, x_j)  = x_i\times x_j
```



#### 高斯核函数

```math
K(x_i, x_j)  = exp(-\frac{||x-y||^2}{2\sigma^2})
```


虽然被广泛使用，但是这个核函数的性能对参数十分敏感，以至于有一大把的文献专门对这种核函数展开研究，同样，高斯核函数也有了很多的变种，如指数核，拉普拉斯核等。

#### Exponential Kernel


```math
K(x_i, x_j)  = exp(-\frac{||x-y||}{2\sigma^2})
```


#### Laplacian Kernel

拉普拉斯核完全等价于指数核，唯一的区别在于前者对参数的敏感性降低，也是一种径向基核函数。



```math
K(x_i, x_j)  = exp(-\frac{||x-y||}{\sigma})
```



#### Sigmoid Kernel

Sigmoid 核来源于神经网络，现在已经大量应用于深度学习，是当今机器学习的宠儿，它是S型的，所以被用作于“激活函数”。关于这个函数的性质可以说好几篇文献，大家可以随便找一篇深度学习的文章看看。


```math
K(x_i, x_j)  = tanh(ax_ix_j + c)
```


