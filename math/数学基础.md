[TOC]
# 信息论
## 自信息 ，信息熵，互信息
### 自信息
概率越大信息商越低 `$I(x)=-log(P(x)) $`
当该对数的底数为自然对数 e 时，单位为奈特（nats）；当以 2 为底数时，单位为比特（bit）或香农（shannons)

### 信息商
对所有事件信息量的期望，对整个概率中所有不确定的量化
```math
H(X) = E_{X}[I(x)]=-\sum_{x \in X} p(x)log(p(x))
```
`$ \quad X $`表示所有事件，信息论中，记 0log0 = 0

当且仅当某个
`$P(X_i)=1$`，其余的都等于0时， H(X)= 0。
当且仅当某个`$P(X_i)=1/n，i=1， 2，……， n$`时，`$H(X)$` 有极大值 log n。

信息商越大，样本的不确定性越大

### 互信商
```math
I(X,Y) = \sum_{y \in Y} \sum_{x \in X} p(x,y) log( \frac{p(x,y)}{p(x)p(y)})
```

互信息 `$I(X,Y)$` 取值为非负。当X、Y相互独立时，`$I(X,Y)$` 最小为0。

---
---
## 相对熵（KL散度） 与 交叉熵
###  相对熵 -- KL 散度 ： Kullback-Leibler divergence
如果对于同一个随机变量 x 有两个单独的概率分布 P(x) 和 Q(x)，我们可以使用 KL 散度来衡量这两个分布的差异。

P 对 Q 的KL散度为:
```math
D_P(Q) =\sum_{x \in X}P(x)log(\frac{P(x)}{Q(x)})
```

含义：

在离散型变量的情况下， KL 散度衡量的是：当我们使用一种被设计成能够使得概率分布 Q 产生的消息的长度最小的编码，发送包含由概率分布 P 产生的符号的消息时，所需要的额外信息量。(粗暴理解：P与Q的信息差)

性质:

KL 散度为 0 当且仅当P 和 Q 在离散型变量的情况下是相同的分布，或者在连续型变量的情况下是“几乎处处”相同的.

不对称：`$D_p(q) != D_q(p)$`

### 交叉商
设 `$p(x), q(x)$` 为 `$X$` 中取值的两个概率分布，则` $p$` 对 `$q$` 的交叉熵为：
```math
D(p || q) = -\sum_{x \in X}p(x)log(\frac{p(x)}{q(x)})
```

在一定程度上，相对熵可以度量两个*随机变量*(原文可能写错，我觉得是两个分布)的“距离”

### 交叉商与KL散度的关系

针对 Q 最小化交叉熵等价于最小化 P 对 Q 的 KL 散度，因为 Q 并不参与被省略的那一项。 `$ H_P(Q) = H(P) + D_P(Q)$`

最大似然估计中，最小化 KL 散度其实就是在最小化分布之间的交叉熵。

---
---
## 联合熵与条件熵
联合熵 `$H(X, Y)$`：两个随机变量X，Y的联合分布的信息商

条件熵 `$H(Y|X) $`：在随机变量X发生的前提下，随机变量Y发生所新带来的熵定义为Y的条件熵，用来衡量在已知随机变量X的条件下随机变量Y的不确定性。

理解类似联合概率/条件概率

`$ H(Y|X) = H(X,Y) - H(X) $`
联合熵与条件熵的推导过程如下：

```math
H(X, Y) - H(X)

= -\sum_{x,y} p(x,y) log(p(x,y))+ \sum_x p(x) log (p(x))

= -\sum_{x,y} p(x,y) log(p(x,y)) + \sum_x \sum_y p(x,y)log (p(x))
```

边缘分布 p(x) 等于联合分布 p(x,y) 的和

```math
= -\sum_{x,y} p(x,y) log (p(x,y) )+ \sum_{x,y} p(x,y) log ( p(x) )

= -\sum_{x,y} p(x,y) log (\frac{p(x,y)}{p(x)} )

= -\sum_{x,y} p(x,y) log (p(y|x))
```

### 互信息
`$I(X, Y)$` ：两个随机变量X，Y的互信息 为X，Y的联合分布和各自独立分布乘积的相对熵。
```math
I(X, Y) = \sum_{x,y} p(x,y) log (\frac{p(x,y)}{p(x)p(y)} )

I(X, Y) = D(P(X,Y) || P(X)P(Y))

I(X,Y)= H(X)+H(Y)-H(X,Y)
```

---
---
---
---
---
# 统计
## 抽样方法

分层抽样的适用范围？

分层抽样利用事先掌握的信息， 充分考虑了保持样本结构和总体结构的一致性，当总体由差异明显的几部分组成的时候，适合用分层抽样。

样本增强

---
---
---
---
---
# 概率论
## 随机事件和概率
### 事件运算规律
交换律： 
`$ A \cup B = B \cup A $` `$ A \cap B = B \cap A $`

结合律：`$ A \cup (B \cup C) = (A \cup B) \cup C $` `$A \cap (B \cap C) = (A \cap B) \cap C \ $`

分配律： `$ A \cap (B \cup C) = (A \cap B) \cup (A \cap C) $`
`$  A \cup (B \cap C) = (A \cup B) \cap (A \cup C) $`

### 条件概率
` $ P(B|A) = \frac{P(AB)}{P(A)} $`

### 独立事件 iid
A B 相互独立 <--> P(AB) = P(A)P(B)

A,B 相互独立的充要条件为 A 与 `$\overline{B} $ `或 `$\overline{A}$` 与 B 或 `$\overline{A}$` 与 `$\overline{B}$` 相互独立。
当 0 < P(A) < 1 时， A, B 相互独立 等价于 P(B|A) = P(B) 或 `$P(B|A) = P(B|\overline{A})$`成立。
n 个事件间相互独立 --> 这n个事件必两两独立； 反之不成立。


### 五大公式

加法公式：
```math
P(A + B) = P(A) + P(B) - P(AB)

P(A + B + C) = P(A) + P(B) + P(C) - P(AB) - P(AC) - P(BC) + P(ABC)
```

减法公式：
```math
P(A - B) = P(A) - P(AB)
```

乘法公式：

```math
P(A) > 0 时， P(AB) = P(A)P(B|A)
```

全概率公式：

```math
P(A) = \sum_{i=1}^n P(B_i)P(A|B_i)
```


贝叶斯公式：

```math
P(B_j| A) = \frac{P(B_j)P(A|B_j)}{\sum_{i=1}^n P(B_i)P(A|B_i)}
```

### 古典型概率

定义： 在样本空间中，有有限 n 个样本点，且每个样本点的发生具有相等的可能性，则称这种有限等可能试验为古典概型。

如果事件 A 由 `$n_A$` 个样本点组成，则事件 A 的概率为： `$ P(A) = \frac{n_A}{n} = \frac{A 中包含的样本点}{样本空间中的样本点总数} $`

### 几何型概率
定义：当试验的样本空间是某区域（该区域可以是一维，二维或三维等）， 以 `$L(\Omega)$` 表示当前样本空间 `$\Omega$` 的几何度量（长度，面积，体积）等。 `$L(\Omega)$` 为有限，且试验结果出现在 `$\Omega$` 中的任意区域的可能性只与该区域几何度量成正比。

如果事件 A 的样本点表示的区域为 `$\Omega_A$` ， 那么事件A的概率为： `$ P(A) = \frac{L(\Omega_A)}{L(\Omega)} = \frac{\Omega_A 的几何度量}{\Omega 的几何度量} $`

### 伯努利试验

随机试验，每次试验都只有两个结果 `$A$` 与 `$\overline{A}$`， 则称为伯努利试验。
n重伯努利试验： 将伯努利试验独立重复进行 n 次， 称为 n 重伯努利试验。

若每次实验中， `P(A)= p$`， 那么 n 重伯努利试验中事件 A 发生 k 次的概率为： `$ 二项概率公式：C_n^k p^k(1-p)^{n-k} $`

---
---
## 随机变量与分布

### 离散型随机变量
`$ 概率分布：P(X=x_k) = p_k  分布函数： F(x) = P(X \leq x) = \sum_{x_k \leq x}p_k \ $`

### 连续型随机变量

积分
`$ F(x) = \int_{-\infty}^{x} f(t)dt $`

---
---
## 常见概率分布
### 几何分布

n重伯努利分布

`$ P（{X = K }） = p (1-p)^{k-1} $`

### 超几何分

不放回实验

N 件商品中含有 M 件次品，从中任意一次取出 n 件(或从中一件接一件不放回的取n件)， 令 X = 抽取的n件商品中的次品件数， 则 X 服从参数为 n， N， M 的超几何分布。

`$ P({ X = k }) = \frac{C_M^kC_{N-M}^{n-k}}{C_N^n} $`


### 泊松分布

一段时间内电话总机接到的呼叫次数， 候车的旅客数，保险索赔的次数都服从泊松分布。 即 一次事件发生不影响事件的再次发生

```math
P({X = k } )= \frac{\lambda^k}{k!} e^{-\lambda};  \quad X \sim P(\lambda)
```

### 均匀分布 uniform
X 在区间 [a,b] 上服从均匀分布，则 `$X \sim U(a,b)$`

```math
f(x)= \begin{cases} \frac{1}{b-a}, & a < x <b \\ 0, & \text{其他} \end{cases}
```

```math
F(x)= \begin{cases} 0, & x < a \\ \frac{x-a}{b-a}, & a \leq x <b \\ 1, & x \geq b \end{cases}
```

### 指数分布

指数分布是描述泊松分布中事件发生时间间隔的概率分布。除了用于泊松过程的分析，还有许多其他应用，如以下场景：世界杯比赛中进球之间的时间间隔

指数分布有如下的适用条件：
1. x是两个事件发生之间的时间间隔，并且x>0;
2. 事件之间是相互独立的；
3. 事件发生的频率是稳定的；
4. 两个事件不能发生在同一瞬间。

指数分布： `$X \sim E(\lambda)$`

指数分布的PDF(概率分布密度)：

```math
f(x)= \begin{cases} λe ^{-λx}, &x>0 \\ 0, & else \end{cases}
```

`$$`


### 正态分布
一维

```math
f(x) = \frac{1}{({2\pi})^{\frac{1}{2}}\sigma} exp(-\frac{{(\mu-x)}^2}{2\sigma^2})
```
标准时，`$\mu = 0, \sigma = 1$`


### beta 分布
```math
Beta(\mu|a,b)

= \frac{\Gamma (a + b)}{\Gamma(a) \Gamma(b)} \mu^{a-1}(1-\mu)^{b-1} \ E(\mu)

= \frac{a}{a+b} \ var(\mu)

= \frac{ab}{(a+b)^2(a+b+1)}
```

---
---
## 随机变量的数学特征
### 期望

离散型随机变量： `$ P(x_k) = p_k \quad E(x) = \sum_{k=1}^n x_k p_k $`

连续型随机变量： `$ E(X) = \int_{-\infty}^{+\infty} xf(x)dx $`

性质：

设 C 为常数, X， Y 为随机变量

```math
E(C) = C

E(CX) = CE(X)

E(X \pm Y) = E(X) \pm E(Y)
```
`$E(XY) = E(X)E(Y)$` 的充要条件为 X , Y不相关。



---



随机变量X的函数 `$Y = g(X)$` 的数学期望：

X 为离散随机变量：
```math
P(X=x_k) = p_k

E(Y) = E(g(X)) = \sum_{k=1}^{n} g(x_k)p_k
```
X 为连续随机变量：

```math
X 概率密度为f(x)；

 E(Y) = E(g(X)) = \int_{-\infty}^{+\infty} g(x)f(x)dx
```
---

随机变量 (X, Y) 的函数 `$Z = g(X, Y)$` 的数学期望：

(X, Y) 为离散随机变量：

```math
P({X=x_i, Y=y_j}) = p_{ij};


 E(Z) = E[g(X,Y)] = \sum_{i=1}^n \sum_{i=1}^m g(x_i, y_j)p_{ij}
```

(X,Y) 为连续随机变量：

```math
概率密度：f(x,y);

\quad \ E(Z) = E[g(X,Y)] = \int_{-\infty}^{+\infty} \int_{-\infty}^{+\infty} g(x,y)f(x,y)dxdy
```
---
---
## 方差
`$ var(X) = E(X^2) - [E(X)]^2 $`

无偏差： `$var(X) = \frac{\sum_{i=1}^n(X-\overline{X})(X-\overline{X})}{n-1}$`

`$var(C) = 0$`时不能反推出 C 为常数

`$var(aX+b) = a^2var(X)$`

`$var(X \pm Y) = var(X) + var(Y)$` 的充要条件是 X 与 Y 不相关。

`$var(-X) = var(X)$`


---
---
## 常见期望与方差
### 伯努利分布

```math
P(X=k) = p^k (1-p)^k, k = 0,1

E(X) = p

D(X) = p - p^2 = p(1-p)
```
### 二次分布

```math
P(X=k) = C_n^k p^k (1-p)^{n-k}

E(X) = np

D(X) = np -np^2 = np(1-p)
```

### 泊松分布

```math
P(X=k) =  \frac{\lambda^k}{k!} e^{-\lambda}

E(X) = \lambda

D(X) = \lambda
```

### 均匀分布

```math
E(X)=\frac{a+b}{2}

D(X) = \frac{(b-1)^2}{12}
```

### 指数分布

```math
f(x)= \begin{cases} λe ^{-λx}, &x>0 \\ 0, & else \end{cases}

E(X) = \frac{1}{\lambda}
E(X) = \frac{1}{\lambda^2}
```

### 正态/高斯分布

```math
E(X)=μ

D(X)= σ^2
```

---
---
## 协方差

定义： 对于随机变量X ,Y ， 如果 `$E{[X - E(x)][Y - E(Y)]}$`存在，则称之为 X 和 Y 的协方差： `$ cov(X,Y) = E{[X - E(x)][Y - E(Y)]} = E(XY) - E(X)E(Y) $`

性质：
```math
cov(X,Y) = E(XY) - E(X)E(Y)

var(X \pm Y) = var(X) + var(Y) \pm 2 cov(X,Y)

cov(X,Y) = cov(Y, X)\ cov(aX, bY) = abcov(X, Y);

a,b 为常数 \ cov(X_1+X_2, Y) = cov(X_1, Y) + cov(X_2, Y)
```
与var比较

```math
var(X) = \frac{\sum_{i=1}^n(X-\overline{X})(X-\overline{X})}{n-1}

cov(X,Y) = \frac{\sum_{i=1}^n(X-\overline{X})(Y-\overline{Y})}{n-1}
```
---
---
## 相关系数

定义： 对于随机变量 X 和 Y， 如果 `$var(X)var(Y) \neq 0$`， 则称 `$\frac{cov(X,Y)}{\sqrt{var(X)} \sqrt{var(Y)}}$` 为 X 与 Y 的相关系数。 `$ \rho_{XY} = \frac{cov(X,Y)}{\sqrt{D(X)} \sqrt{D(Y)}} $`

性质： `$ |\rho_{XY}| \leq 1 \ |\rho_{XY}| = 1   $` 的充要条件为存在不全为0的常数 a，b, 使得：`$\ P(aX+bY =1) = 1 $`

---
---
## 独立与不相关
不相关： 如果随机变量 X 与 Y 的相关系数`$\rho_{XY} = 0$`， 则称 X 与 Y 不相关。

相互独立一定不相关，不相关不一定相互独立

对于二维正态随机变量(X,Y), X 和 Y相互独立的充要条件为 `$\rho = 0$`

对于二维正态随机变量(X,Y)， X,Y相互独立与不相关等价。

---
---
## 贝叶斯定理

### 基本概率
`$P(X=x_i)$` ：边缘概率

`$P(X=x_i,Y=y_i)$`：联合概率

`$P(Y=y_i|X=x_i)$`：条件概率

### 两大规则
加和规则（sum rule）： `$ p(X=x_i) = \sum_{j=1}^L p(X=x_i,Y=y_j) $`

乘积规则（product rule）： `$ p(X=x_i,Y=y_j) = p(Y=y_j|X=x_i)p(X=x_i) $`

### 贝叶斯定理： `$ p(Y|X) = \frac{p(X|Y)p(Y)}{p(X)} $`


---
---
---
---
---

# 线性代数
## 矩阵相乘


```math
c_{ij} = a_{ik} * b_{kj}
```


---
---

## 向量的范数
任意一组向量设为`$\vec{x}=(x_1,x_2,...,x_N)$` 如下：

向量的1范数： 向量的各个元素的绝对值之和
`$ \Vert\vec{x}\Vert_1=\sum_{i=1}^N\vert{x_i}\vert $`

向量的2范数： 向量的每个元素的平方和再开平方根
`$ \Vert\vec{x}\Vert_2=\sqrt{\sum_{i=1}^N{\vert{x_i}\vert}^2} $`

向量的负无穷范数： 向量所有元素的绝对值中最小的 `$ \Vert\vec{x}\Vert_{-\infty}=\min{|{x_i}|} $`

向量的正无穷范数： 向量所有元素的绝对值中最大的

`$ \Vert\vec{x}\Vert_{+\infty}=\max{|{x_i}|} $`

向量的p范数： 向量元素绝对值的p次方和，然后再开P次方根 `$ L_p=\Vert\vec{x}\Vert_p=\sqrt[p]{\sum_{i=1}^{N}|{x_i}|^p} $`

---
---
## 矩阵的范数

对于矩阵 `$A_{m \times n}$`， 举例而说：

```math
A = \begin{bmatrix}
  -1 & 2 & 3 \\
  4 & -6 & 6
  \end{bmatrix}
```
矩阵的1范数（列范数）：矩阵的每一列上的元素绝对值先求和，再从中取个最大值 `$ \Vert A\Vert_1=\max_{1\le j\le n}\sum_{i=1}^m|{a_{ij}}| \ \text{举例}: \Vert A\Vert_1 = max([5,8,9]) = 9 $`

矩阵的2范数： 矩阵 `$A^TA$` 的最大特征值开平方根 `$ \Vert A\Vert_2=\sqrt{\lambda_{max}(A^T A)} $`

---
---
## 特征值分解，特征向量

特征值分解可以得到特征值与特征向量
特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么
矩阵A 的特征值与其特征向量`$\vec{v}$`, 特征值 `$\lambda$` 满足： `$ A\nu = \lambda \nu $`

特征值分解是将一个矩阵分解为如下形式：

`$ A=Q\sum Q^{-1} \ Q: 矩阵A的特征向量组成的矩阵 \ $`

`$\sum$`: 一个对角矩阵，每一个对角元素是一个特征值里面的特征值是由大到小排列的，这些特征值所对应的特征向量就是描述这个矩阵变化方向（从主要的变化到次要的变化排列）。 特征值分解表示矩阵`$A$`的信息可以由其特征值和特征向量表示。

---
---
---
---
---
#  欧拉公式
`$ e^{ix} = cosx + isinx $`
